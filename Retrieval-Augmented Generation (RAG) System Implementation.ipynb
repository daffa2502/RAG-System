{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQy2yv92rA5V"
      },
      "outputs": [],
      "source": [
        "!pip install PyMuPDF pdfminer.six sentence-transformers faiss-cpu elasticsearch langchain gpt4all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjKu_Hotr_KS"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CM73tfMsXUD"
      },
      "outputs": [],
      "source": [
        "doc = fitz.open(\"/content/drive/MyDrive/1706.03762v7.pdf\")  # Replace with the path to your PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBqgxKlfsb68"
      },
      "outputs": [],
      "source": [
        "text = \"\"\n",
        "for page_num in range(10):\n",
        "  page = doc.load_page(page_num)\n",
        "  text += page.get_text(\"text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Hmj-9vws3y7"
      },
      "outputs": [],
      "source": [
        "text = re.sub(r'\\n\\d+\\n', '\\n', text) # remove page number\n",
        "text = re.sub(r'\\n+', '\\n', text) # remove multiple newlines\n",
        "text = re.sub(r'-\\n', '', text) # remove hyphenated line breaks\n",
        "text = re.sub(r'\\[\\s*(\\d+,\\s*)*\\d+\\s*\\]', '', text) #references\n",
        "text = re.sub(r'\\.\\n', '. ## ', text) # for splitting\n",
        "text = re.sub(r'\\n', ' ', text) # remove line break\n",
        "text = re.sub(r'  ', ' ', text) # remove double space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6VkA9baRyOY"
      },
      "outputs": [],
      "source": [
        "chunks = text.split(\"##\")\n",
        "chunks = [chunk.strip() for chunk in chunks if chunk.strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE463MCCbO-O"
      },
      "outputs": [],
      "source": [
        "delete = ['Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗† University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗‡ illia.polosukhin@gmail.com',\n",
        " 'arXiv:1706.03762v7 [cs.CL] 2 Aug 2023',\n",
        " 'Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.',\n",
        " '4To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q · k = Pdk i=1 qiki, has mean 0 and variance dk.',\n",
        " '5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.',\n",
        " 'Model BLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet 23.75 Deep-Att + PosUnk 39.2 1.0 · 1020 GNMT + RL 24.6 39.92 2.3 · 1019 1.4 · 1020 ConvS2S 25.16 40.46 9.6 · 1018 1.5 · 1020 MoE 26.03 40.56 2.0 · 1019 1.2 · 1020 Deep-Att + PosUnk Ensemble 40.4 8.0 · 1020 GNMT + RL Ensemble 26.30 41.16 1.8 · 1020 1.1 · 1021 ConvS2S Ensemble 26.36 41.29 7.7 · 1019 1.2 · 1021 Transformer (base model) 27.3 38.1 3.3 · 1018 Transformer (big) 28.4 41.8 2.3 · 1019 ',\n",
        " 'Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2 · d) O(1) O(1) Recurrent O(n · d2) O(n) O(n) Convolutional O(k · n · d2) O(1) O(logk(n)) Self-Attention (restricted) O(r · n · d) O(1) O(n/r) ',\n",
        " 'Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) WSJ only, discriminative 88.3 Petrov et al. (2006) WSJ only, discriminative 90.4 Zhu et al. (2013) WSJ only, discriminative 90.4 Dyer et al. (2016) WSJ only, discriminative 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al. (2013) semi-supervised 91.3 Huang & Harper (2009) semi-supervised 91.3 McClosky et al. (2006) semi-supervised 92.1 Vinyals & Kaiser el al. (2014) semi-supervised 92.1 Transformer (4 layers) semi-supervised 92.7 Luong et al. (2015) multi-task 93.0 Dyer et al. (2016) generative 93.3 ']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0ebckCseiON"
      },
      "outputs": [],
      "source": [
        "new_chunks = []\n",
        "for chunk in chunks:\n",
        "  for substring in delete:\n",
        "    while substring in chunk:\n",
        "      chunk = chunk.replace(substring, '', 1)\n",
        "      print('deleted: ', substring)\n",
        "  new_chunks.append(chunk)\n",
        "chunks = new_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvUAl7JcV0e5"
      },
      "outputs": [],
      "source": [
        "index_to_remove = [0,2,3,4,5,17,18,48,73,74,75,76,77]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig4b66EoaltS"
      },
      "outputs": [],
      "source": [
        "new_chunks = []\n",
        "for i in range(len(chunks)):\n",
        "  if i not in index_to_remove:\n",
        "    new_chunks.append(chunks[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "join = [4, 10, 14, 19, 21, 28, 36, 45, 59]"
      ],
      "metadata": {
        "id": "ep4FWPLQQ-Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strings = new_chunks\n",
        "for i in range(len(join)):\n",
        "  joined_string = strings[join[i]-i] +\" \"+ strings[join[i]-i+1]\n",
        "  strings = strings[:join[i]-i] + [joined_string] + strings[join[i]-i+2:]\n",
        "  print(joined_string)"
      ],
      "metadata": {
        "id": "HwvgPaBCKpF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_chunks = strings"
      ],
      "metadata": {
        "id": "lUXnR--6WF3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TneIAv59jxVy"
      },
      "outputs": [],
      "source": [
        "for i, chunk in enumerate(new_chunks):\n",
        "  print(f\"{i} \", len(chunk), \" \", chunk,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAy7_5oCtn4-"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "embeddings = embedding_model.encode(new_chunks, convert_to_tensor=True)\n",
        "\n",
        "embeddings = embeddings.cpu()\n",
        "\n",
        "torch.save(embeddings, \"embeddings.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzgBqkK7txk8"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "d = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(embeddings.numpy())\n",
        "\n",
        "query = \"What is the main purpose of the paper?\"\n",
        "query_embedding = embedding_model.encode(query, convert_to_tensor=True).cpu().numpy()\n",
        "\n",
        "k = 5\n",
        "distances, indices = index.search(query_embedding.reshape(1, -1), k)\n",
        "\n",
        "for idx in indices[0]:\n",
        "  print(f\"Chunk {idx}: {new_chunks[idx]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDBol0ediyK-"
      },
      "outputs": [],
      "source": [
        "def relevant_chunks(query, k=5):\n",
        "  query_embedding = embedding_model.encode(query, convert_to_tensor=True).cpu().numpy()\n",
        "  distances, indices = index.search(query_embedding.reshape(1, -1), k)\n",
        "  relevant_chunks = [new_chunks[idx] for idx in indices[0]]\n",
        "  return relevant_chunks\n",
        "\n",
        "query = \"What is an encoder?\"\n",
        "relevant_chunk = relevant_chunks(query, k=5)\n",
        "\n",
        "context = \" \".join(relevant_chunk)\n",
        "print(\"Context for generation:\\n\", context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIXaM1Cxt9WY"
      },
      "outputs": [],
      "source": [
        "from gpt4all import GPT4All\n",
        "\n",
        "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\", allow_download=True)\n",
        "\n",
        "def generate_response(context, query):\n",
        "  prompt = f\"Answer this question: {query}\\n\\nBased on this context: {context}\"\n",
        "  response = model.generate(prompt)\n",
        "  return response\n",
        "\n",
        "response = generate_response(context, query)\n",
        "print(\"Generated Response:\\n\", response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dataset = {\n",
        "    \"What is the Transformer architecture?\":[1,2,3,4,5],\n",
        "    \"Describe the attention mechanism.\":[1,2,3,4,5],\n",
        "    \"Compare the Transformer to traditional RNNs.\":[1,2,3,4,5],\n",
        "    \"What are the key contributions of the paper?\":[1,2,3,4,5],\n",
        "    \"Explain the concept of self-attention.\":[1,2,3,4,5],\n",
        "    \"How does the Transformer handle long-range dependencies?\":[1,2,3,4,5],\n",
        "    \"What are the benefits of using multi-head attention?\":[1,2,3,4,5],\n",
        "    \"Summarize the experiments and results.\":[1,2,3,4,5],\n",
        "    \"What are the main limitations of the Transformer?\":[1,2,3,4,5],\n",
        "    \"How does the Transformer perform compared to other models?\":[1,2,3,4,5]\n",
        "}\n",
        "\n",
        "for query in evaluation_dataset:\n",
        "  print(query)\n",
        "  relevant_chunk = relevant_chunks(query, k=5)\n",
        "  context = \" \".join(relevant_chunk)\n",
        "  print(\"Context for response:\\n\", context)\n",
        "  response = generate_response(context, query)\n",
        "  print(\"Generated Response:\\n\", response)"
      ],
      "metadata": {
        "id": "7PYQY5uTBs2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCpJcbjauFR_"
      },
      "outputs": [],
      "source": [
        "def relevant_indices(query, k=5):\n",
        "  query_embedding = embedding_model.encode(query, convert_to_tensor=True).cpu().numpy()\n",
        "  distances, indices = index.search(query_embedding.reshape(1, -1), k)\n",
        "  relevant_chunks = indices[0]\n",
        "  return relevant_chunks\n",
        "\n",
        "def precision_at_k(relevant_docs, retrieved_docs, k):\n",
        "  retrieved_k = retrieved_docs[:k]\n",
        "  relevant_retrieved = [doc for doc in retrieved_k if doc in relevant_docs]\n",
        "  return len(relevant_retrieved) / k\n",
        "\n",
        "def recall_at_k(relevant_docs, retrieved_docs, k):\n",
        "  retrieved_k = retrieved_docs[:k]\n",
        "  relevant_retrieved = [doc for doc in retrieved_k if doc in relevant_docs]\n",
        "  return len(relevant_retrieved) / len(relevant_docs)\n",
        "\n",
        "for query in evaluation_dataset:\n",
        "  relevant_docs = evaluation_dataset[query]\n",
        "  retrieved_docs = relevant_indices(query, k=5)\n",
        "  k = 5\n",
        "\n",
        "  precision = precision_at_k(relevant_docs, retrieved_docs, k)\n",
        "  recall = recall_at_k(relevant_docs, retrieved_docs, k)\n",
        "  print(f\"Query: {query}\")\n",
        "  print(f\"Precision@{k}: {precision}\")\n",
        "  print(f\"Recall@{k}: {recall}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}